{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_json_en = set(json.load(open('en.json')))\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "stopwords_time = set([\"--\",\"jan\",\"january\",\"feb\",\"february\",\"mar\",\"march\",\"apr\",\"april\",\"may\",\"jun\",\"june\",\"jul\",\"july\",\n",
    "             \"aug\",\"august\",\"sept\",\"september\",\"oct\",\"october\",\"nov\",\"november\",\"dec\",\"december\"])\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct, stopwords_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reuters.fileids()\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "    \n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = [reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "test_categories = [reuters.categories(doc_id) for doc_id in test_docs_id]\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_categories) \n",
    "test_labels = mlb.transform(test_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with stamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stam(text):\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text));\n",
    "    #выкидываем стоп-слова\n",
    "    words = [word for word in words\n",
    "                  if word not in stoplist_combined]\n",
    "    #используем стемминг\n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token),\n",
    "                  words)));\n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    #выкидываем строки с лишними символами и длиной не меньше 3\n",
    "    filtered_tokens =list(filter(lambda token: p.match(token) and len(token)>=min_length,\n",
    "         tokens));\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stam = [tokenize_stam(t) for t in train_docs]\n",
    "x_test_stam = [tokenize_stam(t) for t in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word_tokenize(t) for t in test_docs + train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(t) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "835\n"
     ]
    }
   ],
   "source": [
    "print(np.max(length))\n",
    "print(np.argmin(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13-APR-1987\\n  13-APR-1987\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = test_docs + train_docs\n",
    "all_docs[835]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_onelabel = np.argmax(test_labels, axis = 1)\n",
    "y_train_onelabel = np.argmax(train_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to work svm with tokenised text\n",
    "def const(tmp):\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_x, train_y, test_x, test_y, parameters, pipeline):\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, n_jobs=-1, scoring='accuracy', cv=5, verbose=1)\n",
    "    grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Best parameters set:\")\n",
    "    print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search_tune.best_score_)\n",
    "    print(\"Applying best classifier on test data:\")\n",
    "    best_clf = grid_search_tune.best_estimator_\n",
    "    predictions = best_clf.predict(test_x)\n",
    "    print(accuracy_score(test_y, predictions))\n",
    "    print(classification_report(test_y, predictions))\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.3, max_features=3000, min_df=3,\n",
      "                ngram_range=(1, 1),\n",
      "                preprocessor=<function const at 0x7f12fbbe2290>,\n",
      "                stop_words=None, strip_accents=None,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function const at 0x7f12fbbe2290>, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n",
      "Best score: 0.845\n",
      "Applying best classifier on test data:\n",
      "0.8565750248426631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       719\n",
      "           1       0.60      0.55      0.57        22\n",
      "           2       0.78      0.50      0.61        14\n",
      "           3       0.48      0.67      0.56        30\n",
      "           4       0.57      0.71      0.63        17\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.64      0.82      0.72        17\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.86      0.96      0.91        25\n",
      "          10       0.41      0.73      0.52        15\n",
      "          12       0.53      0.81      0.64        48\n",
      "          13       1.00      0.57      0.73        14\n",
      "          15       0.58      0.62      0.60        24\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.81      0.86      0.84       182\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.52      0.77      0.62        43\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.99      0.98      0.98      1083\n",
      "          22       1.00      0.22      0.36         9\n",
      "          23       1.00      0.33      0.50         9\n",
      "          24       0.34      0.95      0.50        19\n",
      "          25       0.75      0.92      0.83        26\n",
      "          26       0.69      0.68      0.68        77\n",
      "          27       0.00      0.00      0.00         3\n",
      "          29       1.00      0.75      0.86         4\n",
      "          30       1.00      0.50      0.67         4\n",
      "          31       1.00      0.67      0.80         3\n",
      "          32       1.00      0.60      0.75         5\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.80      0.76      0.78       124\n",
      "          35       0.82      0.82      0.82        11\n",
      "          36       0.58      0.50      0.54        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       1.00      0.85      0.92        13\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.00      0.00      0.00        12\n",
      "          41       1.00      1.00      1.00         3\n",
      "          43       0.67      0.33      0.44         6\n",
      "          44       1.00      0.20      0.33         5\n",
      "          45       0.50      0.33      0.40         6\n",
      "          46       0.66      0.61      0.63        96\n",
      "          47       0.82      0.79      0.81        29\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.78      0.54      0.64        13\n",
      "          50       0.00      0.00      0.00         1\n",
      "          54       0.43      0.46      0.44        13\n",
      "          55       1.00      0.89      0.94         9\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       0.80      1.00      0.89         4\n",
      "          59       0.67      0.33      0.44         6\n",
      "          60       0.00      0.00      0.00         3\n",
      "          61       0.00      0.00      0.00         3\n",
      "          62       0.00      0.00      0.00         2\n",
      "          64       0.00      0.00      0.00         1\n",
      "          66       0.90      0.64      0.75        14\n",
      "          67       0.50      1.00      0.67         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "          69       0.67      0.67      0.67         9\n",
      "          71       0.57      0.64      0.60        39\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00         6\n",
      "          78       0.68      0.92      0.78        25\n",
      "          82       0.00      0.00      0.00         3\n",
      "          83       1.00      0.50      0.67        10\n",
      "          84       0.62      0.72      0.67        76\n",
      "          85       0.75      0.27      0.40        11\n",
      "          87       1.00      0.44      0.62         9\n",
      "          88       0.00      0.00      0.00         6\n",
      "          89       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.86      3019\n",
      "   macro avg       0.50      0.44      0.44      3019\n",
      "weighted avg       0.86      0.86      0.85      3019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=const, preprocessor=const)),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])\n",
    "parameters = {\n",
    "                'vect__max_df': [0.3, 0.5, 0,75, 0.9, None],\n",
    "                'vect__min_df': [3, None],\n",
    "                'vect__max_features': [3000, 5000, None],\n",
    "                'vect__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "                #'clf__alpha': [1e-3, 1e-5],\n",
    "} \n",
    "model_bayes_stam_onelabel = grid_search(x_train_stam, y_train_onelabel, x_test_stam, y_test_onelabel, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with Naive bayes: 85.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 10.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
      "                ngram_range=(1, 3),\n",
      "                preprocessor=<function const at 0x7f12fbbe2290>,\n",
      "                stop_words=None, strip_accents=None,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function const at 0x7f12fbbe2290>, vocabulary=None)), ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n",
      "Best score: 0.888\n",
      "Applying best classifier on test data:\n",
      "0.8860549850944022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       719\n",
      "           1       0.88      0.64      0.74        22\n",
      "           2       1.00      0.57      0.73        14\n",
      "           3       0.89      0.80      0.84        30\n",
      "           4       0.65      0.65      0.65        17\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.89      1.00      0.94        17\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       0.33      0.50      0.40         2\n",
      "           9       0.96      0.96      0.96        25\n",
      "          10       0.85      0.73      0.79        15\n",
      "          12       0.84      0.79      0.82        48\n",
      "          13       0.86      0.86      0.86        14\n",
      "          15       0.64      0.67      0.65        24\n",
      "          16       0.50      1.00      0.67         1\n",
      "          17       0.83      0.93      0.88       182\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.66      0.72      0.69        43\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.97      0.99      0.98      1083\n",
      "          22       1.00      0.22      0.36         9\n",
      "          23       0.71      0.56      0.63         9\n",
      "          24       0.58      0.79      0.67        19\n",
      "          25       0.92      0.88      0.90        26\n",
      "          26       0.72      0.79      0.75        77\n",
      "          27       0.00      0.00      0.00         3\n",
      "          29       0.75      0.75      0.75         4\n",
      "          30       1.00      0.50      0.67         4\n",
      "          31       0.75      1.00      0.86         3\n",
      "          32       0.80      0.80      0.80         5\n",
      "          33       1.00      1.00      1.00         1\n",
      "          34       0.77      0.73      0.75       124\n",
      "          35       0.92      1.00      0.96        11\n",
      "          36       0.80      0.57      0.67        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.92      0.85      0.88        13\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       1.00      1.00      1.00         3\n",
      "          43       1.00      0.50      0.67         6\n",
      "          44       1.00      0.60      0.75         5\n",
      "          45       0.75      0.50      0.60         6\n",
      "          46       0.57      0.53      0.55        96\n",
      "          47       0.83      0.83      0.83        29\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.50      0.54      0.52        13\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       0.64      0.69      0.67        13\n",
      "          55       1.00      0.89      0.94         9\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       0.67      1.00      0.80         4\n",
      "          59       0.67      0.33      0.44         6\n",
      "          60       0.00      0.00      0.00         3\n",
      "          61       1.00      0.67      0.80         3\n",
      "          62       0.00      0.00      0.00         2\n",
      "          63       0.00      0.00      0.00         0\n",
      "          64       0.00      0.00      0.00         1\n",
      "          66       1.00      0.79      0.88        14\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "          69       0.90      1.00      0.95         9\n",
      "          71       0.63      0.44      0.52        39\n",
      "          72       0.00      0.00      0.00         0\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00         6\n",
      "          78       0.77      0.92      0.84        25\n",
      "          82       1.00      0.33      0.50         3\n",
      "          83       0.77      1.00      0.87        10\n",
      "          84       0.77      0.88      0.82        76\n",
      "          85       0.57      0.36      0.44        11\n",
      "          87       0.89      0.89      0.89         9\n",
      "          88       0.00      0.00      0.00         6\n",
      "          89       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.89      3019\n",
      "   macro avg       0.62      0.57      0.58      3019\n",
      "weighted avg       0.88      0.89      0.88      3019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('tfidf', CountVectorizer(tokenizer=const, preprocessor=const)),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "parameters = {\n",
    "                'tfidf__max_df': [0.5, 0,75, 0.9],\n",
    "                'tfidf__min_df': [3, None],\n",
    "                'tfidf__max_features': [3000, 5000, None],\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "} \n",
    "model_svc_stam_onelabel = grid_search(x_train_stam, y_train_onelabel, x_test_stam, y_test_onelabel, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with SVM(+CountVectorizer): 88.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
