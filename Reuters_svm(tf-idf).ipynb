{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8ZtGXc_hz_N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwnpAvJehz_V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "9ma7yE3EjDFs",
    "outputId": "2921d925-cec7-4016-b0f1-08bb6b735655",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/alex/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hecFHesLhz_a"
   },
   "outputs": [],
   "source": [
    "stopwords_json_en = set(json.load(open('en.json')))\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "stopwords_time = set([\"--\",\"jan\",\"january\",\"feb\",\"february\",\"mar\",\"march\",\"apr\",\"april\",\"may\",\"jun\",\"june\",\"jul\",\"july\",\n",
    "             \"aug\",\"august\",\"sept\",\"september\",\"oct\",\"october\",\"nov\",\"november\",\"dec\",\"december\"])\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct, stopwords_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sm_n16M2hz_f"
   },
   "outputs": [],
   "source": [
    "documents = reuters.fileids()\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "    \n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJrH1fJFhz_j"
   },
   "outputs": [],
   "source": [
    "train_categories = [reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "test_categories = [reuters.categories(doc_id) for doc_id in test_docs_id]\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_categories) \n",
    "test_labels = mlb.transform(test_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZYwa1vhhz_n"
   },
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GATI6G1bhz_n"
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CwOV9WChz_r"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and word.isalpha() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6yz9W8bhz_v"
   },
   "outputs": [],
   "source": [
    "x_train_lem = [preprocess_text(t) for t in train_docs]\n",
    "x_test_lem = [preprocess_text(t) for t in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VXUeHXBhhz_0",
    "outputId": "ebea7a02-ff15-448e-f129-20d28c706ad4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bahia',\n",
       " 'cocoa',\n",
       " 'review',\n",
       " 'shower',\n",
       " 'continue',\n",
       " 'week',\n",
       " 'bahia',\n",
       " 'cocoa',\n",
       " 'zone',\n",
       " 'alleviate',\n",
       " 'drought',\n",
       " 'early',\n",
       " 'improve',\n",
       " 'prospect',\n",
       " 'temporao',\n",
       " 'normal',\n",
       " 'humidity',\n",
       " 'level',\n",
       " 'restore',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'weekly',\n",
       " 'review',\n",
       " 'dry',\n",
       " 'period',\n",
       " 'temporao',\n",
       " 'late',\n",
       " 'year',\n",
       " 'arrival',\n",
       " 'week',\n",
       " 'end',\n",
       " 'bag',\n",
       " 'kilo',\n",
       " 'make',\n",
       " 'cumulative',\n",
       " 'total',\n",
       " 'season',\n",
       " 'mln',\n",
       " 'stage',\n",
       " 'year',\n",
       " 'cocoa',\n",
       " 'deliver',\n",
       " 'earlier',\n",
       " 'consignment',\n",
       " 'include',\n",
       " 'arrival',\n",
       " 'figure',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'doubt',\n",
       " 'crop',\n",
       " 'cocoa',\n",
       " 'harvesting',\n",
       " 'practically',\n",
       " 'end',\n",
       " 'total',\n",
       " 'bahia',\n",
       " 'crop',\n",
       " 'estimate',\n",
       " 'mln',\n",
       " 'bag',\n",
       " 'sale',\n",
       " 'stand',\n",
       " 'mln',\n",
       " 'hundred',\n",
       " 'thousand',\n",
       " 'bag',\n",
       " 'hand',\n",
       " 'farmer',\n",
       " 'middleman',\n",
       " 'exporter',\n",
       " 'processor',\n",
       " 'doubt',\n",
       " 'cocoa',\n",
       " 'fit',\n",
       " 'export',\n",
       " 'shipper',\n",
       " 'experience',\n",
       " 'dificulties',\n",
       " 'obtain',\n",
       " 'certificate',\n",
       " 'view',\n",
       " 'low',\n",
       " 'quality',\n",
       " 'recent',\n",
       " 'week',\n",
       " 'farmer',\n",
       " 'sell',\n",
       " 'good',\n",
       " 'part',\n",
       " 'cocoa',\n",
       " 'hold',\n",
       " 'consignment',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'spot',\n",
       " 'bean',\n",
       " 'price',\n",
       " 'rise',\n",
       " 'cruzados',\n",
       " 'arroba',\n",
       " 'kilo',\n",
       " 'bean',\n",
       " 'shipper',\n",
       " 'reluctant',\n",
       " 'offer',\n",
       " 'nearby',\n",
       " 'shipment',\n",
       " 'limit',\n",
       " 'sale',\n",
       " 'book',\n",
       " 'shipment',\n",
       " 'dlrs',\n",
       " 'tonne',\n",
       " 'port',\n",
       " 'crop',\n",
       " 'sale',\n",
       " 'light',\n",
       " 'open',\n",
       " 'port',\n",
       " 'dlrs',\n",
       " 'dlrs',\n",
       " 'york',\n",
       " 'dlrs',\n",
       " 'tonne',\n",
       " 'fob',\n",
       " 'routine',\n",
       " 'sale',\n",
       " 'butter',\n",
       " 'make',\n",
       " 'sell',\n",
       " 'dlrs',\n",
       " 'butter',\n",
       " 'time',\n",
       " 'york',\n",
       " 'dlrs',\n",
       " 'dlrs',\n",
       " 'time',\n",
       " 'york',\n",
       " 'dlrs',\n",
       " 'time',\n",
       " 'york',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'destination',\n",
       " 'covertible',\n",
       " 'currency',\n",
       " 'area',\n",
       " 'uruguay',\n",
       " 'open',\n",
       " 'port',\n",
       " 'cake',\n",
       " 'sale',\n",
       " 'register',\n",
       " 'dlrs',\n",
       " 'dlrs',\n",
       " 'dlrs',\n",
       " 'time',\n",
       " 'york',\n",
       " 'buyer',\n",
       " 'argentina',\n",
       " 'uruguay',\n",
       " 'convertible',\n",
       " 'currency',\n",
       " 'area',\n",
       " 'liquor',\n",
       " 'sale',\n",
       " 'limited',\n",
       " 'sell',\n",
       " 'dlrs',\n",
       " 'dlrs',\n",
       " 'time',\n",
       " 'york',\n",
       " 'dlrs',\n",
       " 'time',\n",
       " 'york',\n",
       " 'time',\n",
       " 'york',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'total',\n",
       " 'bahia',\n",
       " 'sale',\n",
       " 'estimate',\n",
       " 'mln',\n",
       " 'bag',\n",
       " 'crop',\n",
       " 'mln',\n",
       " 'bag',\n",
       " 'crop',\n",
       " 'final',\n",
       " 'figure',\n",
       " 'period',\n",
       " 'expect',\n",
       " 'publish',\n",
       " 'brazilian',\n",
       " 'cocoa',\n",
       " 'trade',\n",
       " 'commission',\n",
       " 'carnival',\n",
       " 'end',\n",
       " 'midday']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_lem[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SFuc29Bhz_4"
   },
   "source": [
    "## Tokenization with stamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXYWQvG2hz_5"
   },
   "outputs": [],
   "source": [
    "def tokenize_stam(text):\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text));\n",
    "    #выкидываем стоп-слова\n",
    "    words = [word for word in words\n",
    "                  if word not in stoplist_combined]\n",
    "    #используем стемминг\n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token),\n",
    "                  words)));\n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    #выкидываем строки с лишними символами и длиной не меньше 3\n",
    "    filtered_tokens =list(filter(lambda token: p.match(token) and len(token)>=min_length,\n",
    "         tokens));\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5YsxSKVhz_8"
   },
   "outputs": [],
   "source": [
    "x_train_stam = [tokenize_stam(t) for t in train_docs]\n",
    "x_test_stam = [tokenize_stam(t) for t in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bahia',\n",
       " 'cocoa',\n",
       " 'review',\n",
       " 'shower',\n",
       " 'continu',\n",
       " 'week',\n",
       " 'bahia',\n",
       " 'cocoa',\n",
       " 'zone',\n",
       " 'allevi',\n",
       " 'drought',\n",
       " 'earli',\n",
       " 'improv',\n",
       " 'prospect',\n",
       " 'come',\n",
       " 'temporao',\n",
       " 'normal',\n",
       " 'humid',\n",
       " 'level',\n",
       " 'restor',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'weekli',\n",
       " 'review',\n",
       " 'dri',\n",
       " 'period',\n",
       " 'mean',\n",
       " 'temporao',\n",
       " 'late',\n",
       " 'year',\n",
       " 'arriv',\n",
       " 'week',\n",
       " 'end',\n",
       " 'bag',\n",
       " 'kilo',\n",
       " 'make',\n",
       " 'cumul',\n",
       " 'total',\n",
       " 'season',\n",
       " 'mln',\n",
       " 'stage',\n",
       " 'year',\n",
       " 'cocoa',\n",
       " 'deliv',\n",
       " 'earlier',\n",
       " 'consign',\n",
       " 'includ',\n",
       " 'arriv',\n",
       " 'figur',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'doubt',\n",
       " 'crop',\n",
       " 'cocoa',\n",
       " 'harvest',\n",
       " 'practic',\n",
       " 'end',\n",
       " 'total',\n",
       " 'bahia',\n",
       " 'crop',\n",
       " 'estim',\n",
       " 'mln',\n",
       " 'bag',\n",
       " 'sale',\n",
       " 'stand',\n",
       " 'mln',\n",
       " 'hundr',\n",
       " 'thousand',\n",
       " 'bag',\n",
       " 'hand',\n",
       " 'farmer',\n",
       " 'middlemen',\n",
       " 'export',\n",
       " 'processor',\n",
       " 'doubt',\n",
       " 'cocoa',\n",
       " 'fit',\n",
       " 'export',\n",
       " 'shipper',\n",
       " 'experienc',\n",
       " 'dificulti',\n",
       " 'obtain',\n",
       " 'superior+',\n",
       " 'certif',\n",
       " 'view',\n",
       " 'lower',\n",
       " 'qualiti',\n",
       " 'recent',\n",
       " 'week',\n",
       " 'farmer',\n",
       " 'sold',\n",
       " 'good',\n",
       " 'part',\n",
       " 'cocoa',\n",
       " 'held',\n",
       " 'consign',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'spot',\n",
       " 'bean',\n",
       " 'price',\n",
       " 'rose',\n",
       " 'cruzado',\n",
       " 'arroba',\n",
       " 'kilo',\n",
       " 'bean',\n",
       " 'shipper',\n",
       " 'reluct',\n",
       " 'offer',\n",
       " 'nearbi',\n",
       " 'shipment',\n",
       " 'limit',\n",
       " 'sale',\n",
       " 'book',\n",
       " 'shipment',\n",
       " 'dlr',\n",
       " 'tonn',\n",
       " 'port',\n",
       " 'name',\n",
       " 'crop',\n",
       " 'sale',\n",
       " 'light',\n",
       " 'open',\n",
       " 'port',\n",
       " 'june/juli',\n",
       " 'dlr',\n",
       " 'dlr',\n",
       " 'york',\n",
       " 'aug/sept',\n",
       " 'dlr',\n",
       " 'tonn',\n",
       " 'fob',\n",
       " 'routin',\n",
       " 'sale',\n",
       " 'butter',\n",
       " 'made',\n",
       " 'march/april',\n",
       " 'sold',\n",
       " 'dlr',\n",
       " 'april/may',\n",
       " 'butter',\n",
       " 'time',\n",
       " 'york',\n",
       " 'june/juli',\n",
       " 'dlr',\n",
       " 'aug/sept',\n",
       " 'dlr',\n",
       " 'time',\n",
       " 'york',\n",
       " 'oct/dec',\n",
       " 'dlr',\n",
       " 'time',\n",
       " 'york',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'destin',\n",
       " 'u.s.',\n",
       " 'covert',\n",
       " 'currenc',\n",
       " 'area',\n",
       " 'uruguay',\n",
       " 'open',\n",
       " 'port',\n",
       " 'cake',\n",
       " 'sale',\n",
       " 'regist',\n",
       " 'dlr',\n",
       " 'march/april',\n",
       " 'dlr',\n",
       " 'dlr',\n",
       " 'time',\n",
       " 'york',\n",
       " 'oct/dec',\n",
       " 'buyer',\n",
       " 'u.s.',\n",
       " 'argentina',\n",
       " 'uruguay',\n",
       " 'convert',\n",
       " 'currenc',\n",
       " 'area',\n",
       " 'liquor',\n",
       " 'sale',\n",
       " 'limit',\n",
       " 'march/april',\n",
       " 'sell',\n",
       " 'dlr',\n",
       " 'june/juli',\n",
       " 'dlr',\n",
       " 'time',\n",
       " 'york',\n",
       " 'aug/sept',\n",
       " 'dlr',\n",
       " 'time',\n",
       " 'york',\n",
       " 'oct/dec',\n",
       " 'time',\n",
       " 'york',\n",
       " 'comissaria',\n",
       " 'smith',\n",
       " 'total',\n",
       " 'bahia',\n",
       " 'sale',\n",
       " 'estim',\n",
       " 'mln',\n",
       " 'bag',\n",
       " 'crop',\n",
       " 'mln',\n",
       " 'bag',\n",
       " 'crop',\n",
       " 'final',\n",
       " 'figur',\n",
       " 'period',\n",
       " 'expect',\n",
       " 'publish',\n",
       " 'brazilian',\n",
       " 'cocoa',\n",
       " 'trade',\n",
       " 'commiss',\n",
       " 'carniv',\n",
       " 'end',\n",
       " 'midday']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_stam[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90jVp6zzh0AA"
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bj23PqjCh0AB"
   },
   "outputs": [],
   "source": [
    "def grid_search(train_x, train_y, test_x, test_y, parameters, pipeline):\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, n_jobs=-1, scoring='accuracy', cv=5, verbose=1)\n",
    "    grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Best parameters set:\")\n",
    "    print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search_tune.best_score_)\n",
    "    print(\"Applying best classifier on test data:\")\n",
    "    best_clf = grid_search_tune.best_estimator_\n",
    "    predictions = best_clf.predict(test_x)\n",
    "    print(accuracy_score(test_y, predictions))\n",
    "    print(classification_report(test_y, predictions))\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3wUVQK_h0AE"
   },
   "outputs": [],
   "source": [
    "#to work svm with tokenised text\n",
    "def const(tmp):\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jV-TYKXh0AH"
   },
   "source": [
    "## Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RPP2d4Vah0AI",
    "outputId": "90347995-645d-4d29-afff-0ea41a6b905d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 40.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 62.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 80.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 109.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed: 120.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=5000,\n",
      "                min_df=3, ngram_range=(1, 2), norm='l2',\n",
      "                preprocessor=<function const at 0x7f027b69f830>,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function const at 0x7f027b69f830>, use_idf=True,\n",
      "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight=None, dual=True,\n",
      "                                        fit_intercept=True, intercept_scaling=1,\n",
      "                                        loss='squared_hinge', max_iter=1000,\n",
      "                                        multi_class='ovr', penalty='l2',\n",
      "                                        random_state=None, tol=0.0001,\n",
      "                                        verbose=0),\n",
      "                    n_jobs=-1))]\n",
      "Best score: 0.812\n",
      "Applying best classifier on test data:\n",
      "0.8257701225571381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       719\n",
      "           1       1.00      0.65      0.79        23\n",
      "           2       1.00      0.64      0.78        14\n",
      "           3       0.95      0.63      0.76        30\n",
      "           4       0.83      0.56      0.67        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       1.00      0.94      0.97        18\n",
      "           7       1.00      0.50      0.67         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.93      0.96      0.95        28\n",
      "          10       1.00      0.83      0.91        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.96      0.77      0.85        56\n",
      "          13       1.00      0.55      0.71        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.93      0.50      0.65        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.91      0.90      0.91       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.85      0.75      0.80        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.99      0.98      0.98      1087\n",
      "          22       1.00      0.20      0.33        10\n",
      "          23       1.00      0.53      0.69        17\n",
      "          24       0.93      0.74      0.83        35\n",
      "          25       0.92      0.80      0.86        30\n",
      "          26       0.98      0.84      0.90       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.33      0.50         6\n",
      "          31       1.00      0.50      0.67         4\n",
      "          32       1.00      0.43      0.60         7\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.89      0.72      0.79       131\n",
      "          35       1.00      0.92      0.96        12\n",
      "          36       0.75      0.64      0.69        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       1.00      0.52      0.69        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.00      0.00      0.00        14\n",
      "          41       1.00      1.00      1.00         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.79      0.62      0.70        24\n",
      "          44       1.00      0.17      0.29         6\n",
      "          45       1.00      0.37      0.54        19\n",
      "          46       0.83      0.84      0.84       179\n",
      "          47       0.84      0.79      0.82        34\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.69      0.60      0.64        30\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       1.00      0.17      0.29         6\n",
      "          54       0.78      0.60      0.67        47\n",
      "          55       1.00      0.82      0.90        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.60      0.75        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.00      0.00      0.00        12\n",
      "          60       1.00      0.29      0.44         7\n",
      "          61       0.00      0.00      0.00         3\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       1.00      0.44      0.62         9\n",
      "          66       0.92      0.67      0.77        18\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       0.92      0.50      0.65        24\n",
      "          69       1.00      0.75      0.86        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.94      0.71      0.81        89\n",
      "          72       1.00      0.38      0.55         8\n",
      "          73       0.80      0.40      0.53        10\n",
      "          74       1.00      0.23      0.38        13\n",
      "          75       0.50      0.09      0.15        11\n",
      "          76       0.74      0.52      0.61        33\n",
      "          77       1.00      0.09      0.17        11\n",
      "          78       0.93      0.78      0.85        36\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       1.00      0.20      0.33         5\n",
      "          82       1.00      0.25      0.40         4\n",
      "          83       1.00      0.75      0.86        12\n",
      "          84       0.86      0.73      0.79       117\n",
      "          85       0.94      0.46      0.62        37\n",
      "          86       0.91      0.75      0.82        71\n",
      "          87       1.00      0.60      0.75        10\n",
      "          88       0.33      0.07      0.12        14\n",
      "          89       1.00      0.54      0.70        13\n",
      "\n",
      "   micro avg       0.95      0.82      0.88      3744\n",
      "   macro avg       0.65      0.41      0.48      3744\n",
      "weighted avg       0.92      0.82      0.86      3744\n",
      " samples avg       0.90      0.88      0.88      3744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=const, preprocessor=const)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=-1)),\n",
    "            ])\n",
    "parameters = {\n",
    "                'tfidf__max_df': [0.3, 0.5, 0,75, 0.9],\n",
    "                'tfidf__min_df': [3, None],\n",
    "                'tfidf__max_features': [1000, 3000, 5000, None],\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None],\n",
    "} \n",
    "model_svc_lem = grid_search(x_train_lem, train_labels, x_test_lem, test_labels, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Itn-TFnQh0AL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 40.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 63.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 85.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 118.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed: 130.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
      "                min_df=3, ngram_range=(1, 3), norm='l2',\n",
      "                preprocessor=<function const at 0x7f027b69f830>,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function const at 0x7f027b69f830>, use_idf=True,\n",
      "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight='balanced', dual=True,\n",
      "                                        fit_intercept=True, intercept_scaling=1,\n",
      "                                        loss='squared_hinge', max_iter=1000,\n",
      "                                        multi_class='ovr', penalty='l2',\n",
      "                                        random_state=None, tol=0.0001,\n",
      "                                        verbose=0),\n",
      "                    n_jobs=-1))]\n",
      "Best score: 0.814\n",
      "Applying best classifier on test data:\n",
      "0.8224577674726731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       719\n",
      "           1       0.48      0.57      0.52        23\n",
      "           2       0.90      0.64      0.75        14\n",
      "           3       0.78      0.70      0.74        30\n",
      "           4       0.83      0.56      0.67        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       1.00      0.94      0.97        18\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.96      0.96      0.96        28\n",
      "          10       1.00      0.89      0.94        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.89      0.84      0.86        56\n",
      "          13       0.92      0.55      0.69        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.73      0.57      0.64        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.87      0.93      0.90       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.69      0.82      0.75        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.99      0.98      0.99      1087\n",
      "          22       0.60      0.30      0.40        10\n",
      "          23       1.00      0.59      0.74        17\n",
      "          24       0.94      0.94      0.94        35\n",
      "          25       0.89      0.83      0.86        30\n",
      "          26       0.94      0.91      0.92       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.60      0.60      0.60         5\n",
      "          30       1.00      0.67      0.80         6\n",
      "          31       1.00      0.75      0.86         4\n",
      "          32       1.00      0.43      0.60         7\n",
      "          33       1.00      1.00      1.00         1\n",
      "          34       0.81      0.80      0.80       131\n",
      "          35       0.91      0.83      0.87        12\n",
      "          36       0.79      0.79      0.79        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.93      0.67      0.78        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.50      0.07      0.12        14\n",
      "          41       1.00      1.00      1.00         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.73      0.67      0.70        24\n",
      "          44       1.00      0.17      0.29         6\n",
      "          45       0.91      0.53      0.67        19\n",
      "          46       0.77      0.92      0.83       179\n",
      "          47       0.50      0.79      0.61        34\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.67      0.67      0.67        30\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.04      0.17      0.06         6\n",
      "          54       0.67      0.68      0.67        47\n",
      "          55       1.00      0.82      0.90        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.60      0.75        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       1.00      0.25      0.40        12\n",
      "          60       1.00      0.29      0.44         7\n",
      "          61       1.00      0.67      0.80         3\n",
      "          62       1.00      0.33      0.50         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       1.00      0.44      0.62         9\n",
      "          66       0.86      0.67      0.75        18\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       0.92      0.50      0.65        24\n",
      "          69       1.00      0.75      0.86        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.73      0.80      0.76        89\n",
      "          72       1.00      0.50      0.67         8\n",
      "          73       0.75      0.60      0.67        10\n",
      "          74       0.75      0.23      0.35        13\n",
      "          75       0.40      0.18      0.25        11\n",
      "          76       0.71      0.67      0.69        33\n",
      "          77       0.00      0.00      0.00        11\n",
      "          78       0.97      0.81      0.88        36\n",
      "          79       1.00      1.00      1.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       1.00      0.20      0.33         5\n",
      "          82       1.00      0.50      0.67         4\n",
      "          83       1.00      0.67      0.80        12\n",
      "          84       0.75      0.83      0.79       117\n",
      "          85       0.87      0.54      0.67        37\n",
      "          86       0.85      0.79      0.82        71\n",
      "          87       1.00      0.60      0.75        10\n",
      "          88       0.47      0.50      0.48        14\n",
      "          89       1.00      0.54      0.70        13\n",
      "\n",
      "   micro avg       0.89      0.86      0.87      3744\n",
      "   macro avg       0.65      0.49      0.54      3744\n",
      "weighted avg       0.89      0.86      0.86      3744\n",
      " samples avg       0.90      0.91      0.90      3744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc_stam = grid_search(x_train_stam, train_labels, x_test_stam, test_labels, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for multilabel classification:\n",
    "### SVM with lemmatisation: 0.825\n",
    "\n",
    "### SVM with stamming: 0.822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJgtzHz8h0AO"
   },
   "source": [
    "## Multiclass classification(on target for each text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Здесь автор статьи про RDML https://arxiv.org/pdf/1805.01890v2.pdf сводил  к задаче milticlass следующим образом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bbr_53KWh0AP"
   },
   "outputs": [],
   "source": [
    "y_test_onelabel = np.argmax(test_labels, axis = 1)\n",
    "y_train_onelabel = np.argmax(train_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### По сути из нескольких тем-таргетов выбирался тот, который лексиграфически раньше(если их упорядочить). Не понятно, почему так  можно делать, ведь никакой связи с между ними нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K5dfijXh0AS",
    "outputId": "7d349a8a-783d-40ef-fb65-42d96088d533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 29.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 51.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.9, max_features=5000,\n",
      "                min_df=3, ngram_range=(1, 3), norm='l2',\n",
      "                preprocessor=<function const at 0x7f648d584d40>,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function const at 0x7f648d584d40>, use_idf=True,\n",
      "                vocabulary=None)), ('clf', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n",
      "Best score: 0.893\n",
      "Applying best classifier on test data:\n",
      "0.8973169923815834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       719\n",
      "           1       0.71      0.77      0.74        22\n",
      "           2       1.00      0.64      0.78        14\n",
      "           3       0.86      0.80      0.83        30\n",
      "           4       0.73      0.65      0.69        17\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.85      1.00      0.92        17\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.89      0.96      0.92        25\n",
      "          10       0.78      0.93      0.85        15\n",
      "          12       0.83      0.79      0.81        48\n",
      "          13       1.00      0.79      0.88        14\n",
      "          15       0.74      0.71      0.72        24\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.81      0.97      0.88       182\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.71      0.74      0.73        43\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.99      0.98      0.99      1083\n",
      "          22       1.00      0.22      0.36         9\n",
      "          23       0.80      0.89      0.84         9\n",
      "          24       0.59      0.84      0.70        19\n",
      "          25       0.93      0.96      0.94        26\n",
      "          26       0.76      0.82      0.79        77\n",
      "          27       0.00      0.00      0.00         3\n",
      "          29       1.00      0.75      0.86         4\n",
      "          30       1.00      0.50      0.67         4\n",
      "          31       1.00      0.67      0.80         3\n",
      "          32       1.00      0.80      0.89         5\n",
      "          33       1.00      1.00      1.00         1\n",
      "          34       0.80      0.79      0.79       124\n",
      "          35       0.85      1.00      0.92        11\n",
      "          36       0.77      0.71      0.74        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       1.00      0.85      0.92        13\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       1.00      0.17      0.29        12\n",
      "          41       1.00      1.00      1.00         3\n",
      "          43       0.50      0.50      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.75      0.50      0.60         6\n",
      "          46       0.65      0.65      0.65        96\n",
      "          47       0.86      0.83      0.84        29\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.75      0.69      0.72        13\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       0.60      0.69      0.64        13\n",
      "          55       1.00      1.00      1.00         9\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       0.67      1.00      0.80         4\n",
      "          59       1.00      0.33      0.50         6\n",
      "          60       0.00      0.00      0.00         3\n",
      "          61       1.00      0.67      0.80         3\n",
      "          62       0.00      0.00      0.00         2\n",
      "          64       0.00      0.00      0.00         1\n",
      "          66       1.00      0.79      0.88        14\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "          69       0.89      0.89      0.89         9\n",
      "          71       0.62      0.38      0.48        39\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       1.00      0.17      0.29         6\n",
      "          78       0.83      0.96      0.89        25\n",
      "          82       1.00      0.67      0.80         3\n",
      "          83       0.90      0.90      0.90        10\n",
      "          84       0.74      0.95      0.83        76\n",
      "          85       1.00      0.64      0.78        11\n",
      "          87       0.80      0.89      0.84         9\n",
      "          88       0.00      0.00      0.00         6\n",
      "          89       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.90      3019\n",
      "   macro avg       0.66      0.59      0.60      3019\n",
      "weighted avg       0.89      0.90      0.89      3019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=const, preprocessor=const)),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "parameters = {\n",
    "                'tfidf__max_df': [0.3, 0.5, 0,75, 0.9],\n",
    "                'tfidf__min_df': [3, None],\n",
    "                'tfidf__max_features': [1000, 3000, 5000, None],\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                \"clf__C\": [0.01, 0.1, 1],\n",
    "                \"clf__class_weight\": ['balanced', None],\n",
    "} \n",
    "model_svc_lem_onelabel = grid_search(train_docs_tokens, y_train_onelabel, test_docs_tokens, y_test_onelabel, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Svq6vPxrh0AW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed: 49.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
      "                min_df=3, ngram_range=(1, 2), norm='l2',\n",
      "                preprocessor=<function const at 0x7f027b69f830>,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function const at 0x7f027b69f830>, use_idf=True,\n",
      "                vocabulary=None)), ('clf', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n",
      "Best score: 0.899\n",
      "Applying best classifier on test data:\n",
      "0.8999668764491553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       719\n",
      "           1       0.89      0.73      0.80        22\n",
      "           2       1.00      0.64      0.78        14\n",
      "           3       0.86      0.80      0.83        30\n",
      "           4       0.65      0.65      0.65        17\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.85      1.00      0.92        17\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.89      0.96      0.92        25\n",
      "          10       0.76      0.87      0.81        15\n",
      "          12       0.82      0.77      0.80        48\n",
      "          13       1.00      0.79      0.88        14\n",
      "          15       0.81      0.71      0.76        24\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.81      0.96      0.88       182\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.69      0.72      0.70        43\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.99      0.99      0.99      1083\n",
      "          22       1.00      0.33      0.50         9\n",
      "          23       1.00      0.89      0.94         9\n",
      "          24       0.52      0.79      0.62        19\n",
      "          25       0.89      0.96      0.93        26\n",
      "          26       0.74      0.82      0.78        77\n",
      "          27       0.00      0.00      0.00         3\n",
      "          29       1.00      0.75      0.86         4\n",
      "          30       1.00      0.50      0.67         4\n",
      "          31       1.00      0.67      0.80         3\n",
      "          32       1.00      0.80      0.89         5\n",
      "          33       1.00      1.00      1.00         1\n",
      "          34       0.80      0.81      0.80       124\n",
      "          35       0.85      1.00      0.92        11\n",
      "          36       0.89      0.57      0.70        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       1.00      0.85      0.92        13\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       1.00      0.50      0.67        12\n",
      "          41       1.00      1.00      1.00         3\n",
      "          43       1.00      0.50      0.67         6\n",
      "          44       1.00      0.60      0.75         5\n",
      "          45       0.80      0.67      0.73         6\n",
      "          46       0.69      0.69      0.69        96\n",
      "          47       0.86      0.83      0.84        29\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.78      0.54      0.64        13\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       0.53      0.69      0.60        13\n",
      "          55       1.00      1.00      1.00         9\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       0.67      1.00      0.80         4\n",
      "          59       1.00      0.33      0.50         6\n",
      "          60       0.00      0.00      0.00         3\n",
      "          61       1.00      0.67      0.80         3\n",
      "          62       0.00      0.00      0.00         2\n",
      "          64       0.00      0.00      0.00         1\n",
      "          66       1.00      0.71      0.83        14\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "          69       0.90      1.00      0.95         9\n",
      "          71       0.62      0.41      0.49        39\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00         6\n",
      "          78       0.83      0.96      0.89        25\n",
      "          82       1.00      0.67      0.80         3\n",
      "          83       0.91      1.00      0.95        10\n",
      "          84       0.74      0.95      0.83        76\n",
      "          85       1.00      0.55      0.71        11\n",
      "          87       0.89      0.89      0.89         9\n",
      "          88       0.00      0.00      0.00         6\n",
      "          89       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.90      3019\n",
      "   macro avg       0.66      0.59      0.61      3019\n",
      "weighted avg       0.89      0.90      0.89      3019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=const, preprocessor=const)),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "parameters = {\n",
    "                'tfidf__max_df': [0.5, 0,75, 0.9],\n",
    "                'tfidf__min_df': [3, None],\n",
    "                'tfidf__max_features': [3000, 5000, 7000, None],\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                \"clf__C\": [0.01, 1],\n",
    "                \"clf__class_weight\": ['balanced', None],\n",
    "} \n",
    "model_svc_stam_onelabel = grid_search(x_train_stam, y_train_onelabel, x_test_stam, y_test_onelabel, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WATaTqBLh0AZ"
   },
   "source": [
    "## Accuracy for multiclass classification:\n",
    "### SVM with lemmatisation: 0.893\n",
    "\n",
    "### SVM with stamming: 0.899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Reuters_svm_tune.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
